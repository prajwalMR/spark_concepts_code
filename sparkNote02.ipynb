{"cells":[{"cell_type":"code","source":["stringJSONRDD = sc.parallelize((\"\"\" \n  { \"id\": \"123\",\n    \"name\": \"Katie\",\n    \"age\": 19,\n    \"eyeColor\": \"brown\"\n  }\"\"\",\n   \"\"\"{\n    \"id\": \"234\",\n    \"name\": \"Michael\",\n    \"age\": 22,\n    \"eyeColor\": \"green\"\n  }\"\"\", \n  \"\"\"{\n    \"id\": \"345\",\n    \"name\": \"Simone\",\n    \"age\": 23,\n    \"eyeColor\": \"blue\"\n  }\"\"\")\n)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["stringJSONDF = spark.read.json(stringJSONRDD);"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["stringJSONDF.show();"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["stringJSONDF.createOrReplaceTempView('stringJSON');"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM stringJSON WHERE age > 20\").show();"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')]);"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nschemaString = \"id name age eyeColor\"\nschema = StructType([\n    StructField(\"id\", LongType(), True),    \n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", LongType(), True),\n    StructField(\"eyeColor\", StringType(), True)\n])\n\n# Apply the schema to the RDD and Create DataFrame\nswimmers = spark.createDataFrame(stringCSVRDD, schema)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["swimmers.show();"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["swimmers.printSchema();"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql import Row\n\n# Create Example Data - Departments and Employees\n\n# Create the Departments\ndepartment1 = Row(id='123456', name='Computer Science')\ndepartment2 = Row(id='789012', name='Mechanical Engineering')\ndepartment3 = Row(id='345678', name='Theater and Drama')\ndepartment4 = Row(id='901234', name='Indoor Recreation')\n\n# Create the Employees\nEmployee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\nemployee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\nemployee11 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 200000)\nemployee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\nemployee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\nemployee31 = Employee('matei', None, 'no-reply@waterloo.edu', 180000)\nemployee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n\n# Create the DepartmentWithEmployees instances from Departments and Employees\ndepartmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\ndepartmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4, employee11])\ndepartmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4, employee31])\ndepartmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2 , departmentWithEmployees3 , departmentWithEmployees4]\ndf1 = sqlContext.createDataFrame(departmentsWithEmployeesSeq1)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["display(df1);"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n\ndf = df1.select(\"department\",explode(\"employees\").alias(\"emp\"))\ndisplay(df);"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(df.selectExpr('department.*' , 'emp.*'));"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["df = spark.createDataFrame([\n        (1, 144.5, 5.9, 33, 'M'),\n        (2, 167.2, 5.4, 45, 'M'),\n        (3, 124.1, 5.2, 23, 'F'),\n        (4, 144.5, 5.9, 33, 'M'),\n        (5, 133.2, 5.7, 54, 'F'),\n        (3, 124.1, 5.2, 23, 'F'),\n        (5, 129.2, 5.3, 42, 'M'),\n    ], ['id', 'weight', 'height', 'age', 'gender'])\ndf.show()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["df.drop_duplicates();\ndf.show();"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["df.drop_duplicates(subset = ['weight']);\ndf.show();"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["df_miss = spark.createDataFrame([\n        (1, 143.5, 5.6, 28,   'M',  100000),\n        (2, 167.2, 5.4, 45,   'M',  None),\n        (3, None , 5.2, None, None, None),\n        (4, 144.5, 5.9, 33,   'M',  None),\n        (5, 133.2, 5.7, 54,   'F',  None),\n        (6, 124.1, 5.2, None, 'F',  None),\n        (7, 129.2, 5.3, 42,   'M',  76000),\n    ], ['id', 'weight', 'height', 'age', 'gender', 'income'])"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["df_miss.rdd.map(lambda row : sum([c == None for c in row]) ).collect()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql.functions import count\n\ndf_miss.agg(*[count(c) for c in df_miss.columns]).show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df_miss.rdd.map(lambda x : [str(c).isdigit() == True for c in x] ).collect()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n\ntitanicData = spark.read.csv('/FileStore/tables/titanic_train-ac800.csv' , header = True , inferSchema=True);"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["def getLen(name):\n  return len(name)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\n\nlen_udf = udf(getLen , IntegerType());"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["titanic_data_len = titanicData.select(\"*\" , len_udf(\"Name\").alias(\"name_len\"));"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["titanic_data_len.agg({\"name_len\":\"max\"}).collect()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["def isFemale(name):\n  if 'Mrs' in name or 'Miss' in name:\n    return True;\n  else :\n    return False;"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["from pyspark.sql.types import BooleanType\nisFemale_udf = udf(isFemale , BooleanType());"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["titanic_data_female = titanicData.select(\"Name\" , isFemale_udf(\"Name\").alias(\"isFemale\")).show();"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["from pyspark.sql.functions import pandas_udf , PandasUDFType\n\nimport pandas as pd\nfrom scipy import stats"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["@pandas_udf('double' , PandasUDFType.SCALAR)\ndef pandas_cdf(v):\n  return pd.Series(stats.norm.cdf(v));"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["df.show();"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["df = spark.createDataFrame([('a', 2,3) , ('b', 4,5) , ('c', 6,7) , ('d', 8,9) , ('e', 10,11)] , ['id' , 'c1' , 'c2']);"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["df.show();"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["@pandas_udf(df.schema , PandasUDFType.GROUPED_MAP)\ndef find_mean(pdf):\n  print(type(pdf));\n  return pdf.assign(v = pdf.v - pdf.id)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["@pandas_udf(df.schema , PandasUDFType.GROUPED_MAP)\ndef sub_mean(pdf):\n  print(type(pdf));\n  return pdf.assign(v = min(pdf.c1 , pdf.c2) )"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["titanicData_limit = spark.read.csv('/FileStore/tables/titanic_train-ac800.csv' , header = True , inferSchema=True).limit(20);"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["titanicData_limit.show();"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":40}],"metadata":{"name":"testNote02","notebookId":163173216390944},"nbformat":4,"nbformat_minor":0}
